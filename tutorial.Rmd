---
title: "Introduction to Machine Learning in R (K Nearest Neighbours Algorithm)"
author: "Samuel Kellerhals"
date: "28/08/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## What is Machine Learning?

Machine learning can be understand as algorithms that make data driven predictions and decisions by constructing a model from sample inputs. It is very closely related to computational statistics and draws a lot upon mathematical optimisation. The applications for it are vast, from filtering spam from your emails to advanced computer vision and image recognition  algorithms used for self-driving cars.  


A widely quoted, more formal definition of the algorithms studied in the machine learning field is: 


> "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."


Machine learning can be split into various subdomains. These include: 

- Unsupervised classification

- Supervised classification

- Deep learning algorithms

- Reinforcement learning

- and many more (see more here "insert link")

Today we will focus on supervised classification, and explore one of the simplest and most well understood machine learning tools, The k-nearest neighbour algorithm.  

## The K-nearest neighbour algorithm (k-nn)

K-nn is an example of a supervised classification method, which means we need to first feed it data so it is able to make a classification based on that data (this is called the training phase). Upon training the algorithm on data we provided, we can test our model on an unseen dataset (where we know what class each observation belongs to), and can then see how successful our model is at predicting the classes. This process of first building or selecting a classifier, training it and subsequently testing it is very widespread across the machine learning field and is what you will be doing today. 

### Under the hood

K-nn is a non-parametric technique that stores all available cases and classifies new cases based on a similiarty measure (distance function). Therefore when classifying an unseen dataset using a trained k-nn algorithm, it looks through the training data and finds the *k* training examples that are closest to the new example. It then assigns a class label to the new example based on a majority vote between those *k* training examples. This means if K is equal to 1, the class label will be assigned based on the nearest neighbour. However if K is equal to 3, the algorithm will select the three closest data points to each case and classify it based on a majority vote based on the classes that those three adjacent points hold. 

*insert picture here of K = 1 and K = 3*

You can see that the selection of K is quite important, as is the selection of your training data, because this is all your predictive model will be based on.
Regarding K in general in binary cases it is best to pick an odd K to avoid ties between neigbours. Slightly higher K values can also act to reduce noise in datasets. However it is best to experiment with different K values and use cross validation techniques to find the best value for your specific case (link here for cross validation).
  
## Getting started

Firstly navigate to the repository for this tutorial at (insert git-hub link), and clone it to your local machine. To do so, either click on download the repository contents or alternatively if you have git running on your machine do so by opening the command-prompt or terminal and once you are in your desired working directory type:

```{}
git clone 'repository name'
```

Today we will use the following packages, go ahead and install them if you haven't aready, then load them.

```{r}
# Example of installing a package
#install.packages('ggplot2')

# Required packages for this tutorial
library(ggplot2)
library(dplyr)
library(class)
library(gridExtra)
```

## Getting familiar with our data

### Loading our data

For this tutorial we will be using the built-in Iris UCU Machine Learning dataset. This dataset contains 150 records describing structural traits such as Sepal Length and Petal Width of the Iris genus across three different species. In order to start learning something from our data, it is first important that we familiarise ourselves with it first.

```{r}
# Loading iris dataset
iris.data <- iris

# Viewing iris dataset structure and attributes
str(iris.data)
```

### Data visualisation

We can also visualise our data to understand whether there are any apparent trends. Often exploring our data this way will yield an even better understanding of any underlying relationships we may want to explore further using Machine Learning algorithms such as the k-nn. 

```{r}
ggplot(iris.data, aes(x = Petal.Width, y = Petal.Length, color = Species)) +
  geom_point() +
  theme_classic()
```
```{r}
ggplot(iris.data, aes(x = Sepal.Width, y = Sepal.Length, color = Species)) +
  geom_point() +
  theme_classic()
```

From the above plots we can clearly see a visual correlation between plant traits. These traits also look like they vary greatly between the three iris species. This is also confirmed when looking at the correlation between traints.

```{r}
# Correlation between Sepal Length and Sepal Width
cor(iris$Petal.Length, iris$Petal.Width)
```

The goal of this tutorial therefore will be to use a random sample of this dataset to create a predictive model (k-nn algorithm) which we will use to try to predict what species 'unclassified' data points belong to.

## Building our k-nn classifier

### Data normalisation and training/test-set generation

- normalisation of data (why is it important)

```{r}
# Building a normalisation function

normalise <- function(x) {
  num <- x - min(x)
  denom <- max(x) - min(x)
  return (num/denom)
}
```

Now we normalise all the data columns in the iris dataset. 

```{r}
iris.normalised <- as.data.frame(lapply(iris[1:4], normalise))
```

Now that our data is normalised, we are going to randomly generate our training and test datasets and their respective labels.

```{r}
# Generating random number
set.seed(1234)

# Randomly generating our training and test sampels with a ratio of 2/3 and 1/3
datasample <- sample(2, nrow(iris), replace = TRUE, prob = c(0.67, 0.33))

# Generate training set
iris.training <- iris[datasample == 1, 1:4]

# Generate training labels
irisTraining.labels <- iris[datasample == 1, 5]

# Generate test set 
iris.test <- iris[datasample == 2, 1:4]

# Generate test labels
irisTest.labels <- iris[datasample == 1, 5]
```

It's now time to build our classifier! For this we will use the knn function and pass it our training and test data as well as our training labels. Note that we also select a value for K, which in this case is three as we have two classes and by chosing 3 avoid a tie between the two classes.

```{r}
iris.knn <- knn(train = iris.training, test = iris.test, cl = irisTraining.labels,
                k = 3)
```

Next we need to evaluate the performance of our model. To do this we want to find out if the classes our model predicts based on the training data are able to accurately predict the species class of our iris dataset. For this we need to compare the labels of our iris test dataset to the predictions made by our knn algorithm.

```{r}
# creating a dataframe from known test labels
test.labels <- data.frame(irisTest.labels)

# combining predicted and known species classes
class.comparison <- data.frame(iris.knn, test.labels)

# giving appropriate column names
names(class.comparison) <- c("Predicted Species", "Observed Species")
```

Finally we can evaluate the model using a confusion matrix.

```{r}
CrossTable(x = iris.testLabels, y = iris.knn, prop.chisq = FALSE)
```


-Limitations: quite simple algorithm, euclidean distance not good in higher dimensions (when more features are present). Sometimes require preprocessing steps such as PCA dimensionality reduction techniques followed by knn. 

-However beware that as with all statistics, machine learning is no different when it comes to the importance of working with a representative and reliable dataset. Also remember that the use of an algorithm does not imply that your prediction will always be accurate. Therefore before implementing more complex models and algorithms it is important that you understand how they function and what implications that may have on your results. 
